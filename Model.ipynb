{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script implements various deep-neural LSTM architectures (Unidirectional, Bidirectional, Target-dependent LSTM) with different word embeddings for sarcasm target detection. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense ,LSTM,concatenate,Input,Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['Snippet', 'Target'] \n",
    "df = pd.read_csv('dataset.csv', names = colnames, header = None)\n",
    "ppd=pd.read_csv('pre_processed_dataset.csv', encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(b\"Data_fast.pkl\",\"rb\")\n",
    "keras_left_context, keras_right_context, keras_middle, labels = zip(*pickle.load(f))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(b\"Data_aug.pkl\",\"rb\")\n",
    "pos_vec, ner_vec ,empath_vec = zip(*pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5913,)\n",
      "(5913,)\n",
      "(5913, 300)\n",
      "(5913, 1, 45)\n",
      "(5913, 1, 4)\n",
      "(5913, 194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(keras_left_context))\n",
    "print(np.shape(keras_right_context))\n",
    "print(np.shape(keras_middle))\n",
    "print(np.shape(pos_vec))\n",
    "print(np.shape(ner_vec))\n",
    "print(np.shape(empath_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned-Hyper Parameters\n",
    "embed_size = 1024\n",
    "hidden_size = 32\n",
    "num_epochs=30\n",
    "layer_size = 16\n",
    "batch_size = 64\n",
    "# 'Uni' : Unidirectional LSTM  |  'Bi' : Bidirectional LSTM  | 'TD' : Target-dependent LSTM\n",
    "mode = 'Uni' \n",
    "augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compress candidate-words belonging to the same tweet/line togerather\n",
    "def compress():\n",
    "    lengths = []\n",
    "    for i in range (1,len(ppd)):\n",
    "        if ppd['left_context'][i][2:-2] == \"<start>\":\n",
    "            lengths.append(i)\n",
    "    lengths.append(len(ppd))\n",
    "    compressor = []\n",
    "    compressor.append(range(lengths[0]))\n",
    "    for i in range (1,len(lengths)):\n",
    "        compressor.append(range(lengths[i-1],lengths[i]))\n",
    "    return compressor\n",
    "comp = compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset division for 3-fold cross validation\n",
    "indices = list (range (len(comp)))\n",
    "np.random.shuffle(indices)\n",
    "bins = []\n",
    "bins.append(indices[:int(0.33*len(indices))])\n",
    "bins.append(indices[int(0.33*len(indices)):int(0.66*len(indices))])\n",
    "bins.append(indices[int(0.66*len(indices)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data, Test Data Preparation\n",
    "\n",
    "def prep (train_indices, test_indices):\n",
    "    print (len(train_indices), len(test_indices))\n",
    "\n",
    "    train_ids = []\n",
    "    for i in range(len(train_indices)):\n",
    "        train_ids.extend(comp[train_indices[i]])\n",
    "\n",
    "    test_ids = []\n",
    "    for i in range(len(test_indices)):\n",
    "        test_ids.extend(comp[test_indices[i]])\n",
    "\n",
    "    # Training Data Preparion\n",
    "    train_left   = []\n",
    "    train_right  = []\n",
    "    train_middle = []\n",
    "    train_labels = []\n",
    "    train_pos_vec= []\n",
    "    train_ner_vec= []\n",
    "    train_empath_vec=[]\n",
    "\n",
    "    for id in train_ids:\n",
    "        train_left.append(keras_left_context[id])\n",
    "        train_right.append(keras_right_context[id])\n",
    "        train_middle.append(keras_middle[id])\n",
    "        train_labels.append(labels[id])\n",
    "        train_pos_vec.append(pos_vec[id])\n",
    "        train_ner_vec.append(ner_vec[id])\n",
    "        train_empath_vec.append(empath_vec[id])\n",
    "\n",
    "    train_left   = np.array(train_left)\n",
    "    train_right  = np.array(train_right)\n",
    "    train_middle = np.array(train_middle)\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_middle = np.expand_dims(train_middle,axis=1)\n",
    "    train_pos_vec= np.array(train_pos_vec)\n",
    "    train_ner_vec= np.array(train_ner_vec)\n",
    "    train_empath_vec= np.array(train_empath_vec)\n",
    "\n",
    "    # Training Data Preparion\n",
    "    val_left   = []\n",
    "    val_right  = []\n",
    "    val_middle = []\n",
    "    val_labels = []\n",
    "    val_pos_vec= []\n",
    "    val_ner_vec= []\n",
    "    val_empath_vec=[]\n",
    "\n",
    "    for id in test_ids:\n",
    "        val_left.append(keras_left_context[id])\n",
    "        val_right.append(keras_right_context[id])\n",
    "        val_middle.append(keras_middle[id])\n",
    "        val_labels.append(labels[id])\n",
    "        val_pos_vec.append(pos_vec[id])\n",
    "        val_ner_vec.append(ner_vec[id])\n",
    "        val_empath_vec.append(empath_vec[id])\n",
    "\n",
    "    val_left   = np.array(val_left)\n",
    "    val_right  = np.array(val_right)\n",
    "    val_middle = np.array(val_middle)\n",
    "    val_labels = np.array(val_labels)\n",
    "    val_middle = np.expand_dims(val_middle, axis=1)\n",
    "    val_pos_vec=np.array(val_pos_vec)\n",
    "    val_ner_vec=np.array(val_ner_vec)\n",
    "    val_empath_vec=np.array(val_empath_vec)\n",
    "\n",
    "    # Below part only for TD lstm\n",
    "    if mode == 'TD':  \n",
    "        train_left = np.concatenate((train_left, train_middle), axis=1)\n",
    "        train_right = np.concatenate((train_middle, train_right), axis=1)\n",
    "        val_left = np.concatenate((val_left, val_middle), axis=1)\n",
    "        val_right = np.concatenate((val_middle, val_right), axis=1)\n",
    "    return(train_left,train_right,train_middle,train_pos_vec,train_ner_vec, train_empath_vec,train_labels,val_left,val_right,val_middle,val_pos_vec,val_ner_vec, val_empath_vec,val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_comp(arr, test_indices):\n",
    "    arr = deque(arr)\n",
    "    fin = []\n",
    "    for i in test_indices:\n",
    "        temp = []\n",
    "        for j in range(len(comp[i])):\n",
    "            temp.append(arr.popleft())\n",
    "        fin.append(temp)\n",
    "    return(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy (pred, labels, test_indices):\n",
    "    pred = pred[0]\n",
    "    num_sent = len(comp)\n",
    "    num_words = len(pred)\n",
    "    threshold = 0\n",
    "    cnt = 0\n",
    "    # Threshold calculation for binary classification problem\n",
    "    for a,b in zip (pred,labels):\n",
    "        if b==1.0:\n",
    "            threshold+=a\n",
    "            cnt+=1\n",
    "    threshold = threshold.item()/cnt\n",
    "\n",
    "    pred_th = []\n",
    "    for x in pred:\n",
    "        if (x<=threshold):\n",
    "            pred_th.append(0)\n",
    "        else :\n",
    "            pred_th.append(1)\n",
    "\n",
    "    pred_th = np.array(pred_th)\n",
    "    print (\"Number of Test sentences : {}\".format(len(test_indices)))\n",
    "    error = pred_th-labels\n",
    "    error_d  = de_comp(error,test_indices)\n",
    "    labels_d = de_comp(labels,test_indices)\n",
    "    pred_d   = de_comp(pred_th,test_indices)\n",
    "    em_cnt = 0\n",
    "    ds_cnt = 0\n",
    "    mic_f1 = 0\n",
    "\n",
    "    for err in error_d:\n",
    "        if (sum(err)==0):\n",
    "            em_cnt += 1\n",
    "        ds_cnt += float(len(err)-sum(np.abs(err)))/len(err)\n",
    "\n",
    "    for lab, pre in zip(labels_d,pred_d):\n",
    "\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tn = 0\n",
    "              \n",
    "        for i,j in zip(lab,pre):\n",
    "            if (int(i) == 1)  and (int(j) ==0) :\n",
    "                fn += 1\n",
    "            elif (int(i) == 0)  and (int(j) ==1) :\n",
    "                fp += 1\n",
    "            elif (int(i) == 0)  and (int(j) ==0) :\n",
    "                tn += 1\n",
    "            elif (int(i) == 1) and (int(j) ==1):\n",
    "                tp += 1\n",
    "        try : \n",
    "            mic_f1 += float(2*tp) / (2*tp + fn +fp)\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for a,b in zip(labels, pred_th):\n",
    "\n",
    "        if int(a)==0 and b==0:\n",
    "            TN+=1\n",
    "        if int(a)==1 and b==1:\n",
    "            TP+=1\n",
    "        if int(a)==0 and b==1:\n",
    "            FP+=1\n",
    "        if int(a)==1 and b==0:\n",
    "            FN+=1 \n",
    "    print (\"TP = {}, TN = {},FP = {}, FN = {}\".format(TP,TN,FP,FN))\n",
    "    F1 = float(2*TP)/(2*TP + FP+ FN)\n",
    "    EM = float(em_cnt)/len(test_indices)\n",
    "    DS = float(ds_cnt)/len(test_indices)\n",
    "    uF1= float(mic_f1)/len(test_indices)\n",
    "    print (\"EM Accuracy : {}\".format(EM))\n",
    "    print (\"DS Accuracy : {}\".format(DS))\n",
    "    print (\"Micro F1    : {}\".format(uF1))\n",
    "    print (\"Macro F1 Score = {}\".format(F1))\n",
    "    return (pred_d, labels_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_l,train_r,train_m,train_pos,train_ner,train_empath,train_labels,test_l,test_r,test_m,test_pos,test_ner,test_empath,test_labels,test_indices):\n",
    "    \n",
    "    x=Input(shape=(None,embed_size))\n",
    "    y=Input(shape=(None,embed_size))\n",
    "    z=Input(shape=(None,embed_size))\n",
    "    z1=Input(shape=([45]))\n",
    "    z2=Input(shape=([4]))\n",
    "    z4=Input(shape=([194]))\n",
    "\n",
    "    if mode == 'Bi':\n",
    "        left_out=Bidirectional(LSTM(hidden_size//2,return_sequences=False),input_shape=(train_l.shape[1:]))(x)      \n",
    "        middle = Bidirectional(LSTM(hidden_size//2,return_sequences=False),input_shape=(train_m.shape[1:]))(y)\n",
    "        right_out=Bidirectional(LSTM(hidden_size//2,return_sequences=False),input_shape=(train_r.shape[1:]))(z)\n",
    "\n",
    "    else:\n",
    "        left_out  = LSTM(hidden_size,return_sequences=False)(x)\n",
    "        middle    = LSTM(hidden_size,return_sequences=False)(y)\n",
    "        right_out = LSTM(hidden_size,return_sequences=False)(y)\n",
    "\n",
    "    pos_dense=Dense(32,activation='relu')(z1)\n",
    "    ner_dense=Dense(16,activation='relu')(z2)\n",
    "    empath_dense=Dense(64,activation='relu')(z4)\n",
    "\n",
    "    if mode == 'TD' and augmentation == False :\n",
    "        out=concatenate([left_out,right_out],axis=-1)\n",
    "\n",
    "    if mode == 'TD' and augmentation == True :\n",
    "        out=concatenate([left_out,right_out,pos_dense,ner_dense,empath_dense],axis=-1)\n",
    "\n",
    "    if mode != 'TD' and augmentation == False :\n",
    "        out=concatenate([left_out,middle,right_out],axis=-1)\n",
    "\n",
    "    if mode != 'TD' and augmentation == True :\n",
    "        out=concatenate([left_out,middle,right_out,pos_dense,ner_dense,empath_dense],axis=-1)\n",
    "\n",
    "    out=Dense(layer_size, activation='relu')(out)\n",
    "    output=Dense(1, activation='sigmoid')(out)\n",
    "    model = Model(inputs=[x,y,z,z1,z2,z4], outputs=output)\n",
    "    model.compile(optimizer=Adam(lr=10e-5),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    print (\"Starting Epochs\")\n",
    "    for i in range(num_epochs):\n",
    "        model.fit([train_l,train_r,train_m,train_pos,train_ner,train_empath],train_labels,batch_size=batch_size, epochs=1,verbose=0)\n",
    "        print('***************************************************************')\n",
    "        print (\"predicting_ Epoch : {}\".format(i))\n",
    "        pred_val=[]\n",
    "        pred_val.append(model.predict([test_l,test_r,test_m,test_pos,test_ner,test_empath]))\n",
    "        pre_d, lab_d = accuracy (pred_val, test_labels,test_indices)\n",
    "\n",
    "        with open('Tweets Aug-{}.csv'.format(i), mode='w') as file:\n",
    "            file_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            for a,b in zip (pre_d,lab_d):\n",
    "                file_writer.writerow([a,b])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "0 1\n",
      "0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-dbc2bc921b37>:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val_left   = np.array(val_left)\n",
      "<ipython-input-10-dbc2bc921b37>:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val_right  = np.array(val_right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epochs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expect x to be a non-empty array or dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c0309eee9cfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_left\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_right\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_middle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_vec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mner_vec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mempath_vec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_left\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_right\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_middle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_vec_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mner_vec_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mempath_vec_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mSar_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_left\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_right\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_middle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_vec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mner_vec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mempath_vec_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_left\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_right\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_middle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_vec_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mner_vec_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mempath_vec_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mSar_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bert Tweets Aug.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved model to disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-3aedcd88bb26>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(train_l, train_r, train_m, train_pos, train_ner, train_empath, train_labels, test_l, test_r, test_m, test_pos, test_ner, test_empath, test_labels, test_indices)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting Epochs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_r\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_m\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_ner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_empath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'***************************************************************'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"predicting_ Epoch : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ]
    }
   ],
   "source": [
    "for i in range (3):\n",
    "    print (\"Fold {}\".format(i+1))\n",
    "    print (len(bins[0] + bins[1]),len(bins[2]))\n",
    "    train_left,train_right,train_middle,pos_vec_train,ner_vec_train,empath_vec_train,train_labels,val_left,val_right,val_middle,pos_vec_val,ner_vec_val,empath_vec_val,val_labels = prep (bins[i%3] + bins[(i+1)%3], bins[(i+2)%3])\n",
    "    Sar_model=model(train_left,train_right,train_middle,pos_vec_train,ner_vec_train,empath_vec_train,train_labels,val_left,val_right,val_middle,pos_vec_val,ner_vec_val,empath_vec_val,val_labels,bins[(i+2)%3])\n",
    "    Sar_model.save_weights(\"Bert Tweets Aug.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
